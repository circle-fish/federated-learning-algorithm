%%
% 参考文献
% 参考文献是毕业设计(论文)不可缺少的组成部分,它反映毕业设计(论文)的取材来源、材料的广博程度和材料的可靠程度,也是作者对他人知识成果的承认和尊重。一份完整的参考文献可向读者提供一份有价值的信息资料,列入的文献应在 10 篇以上,其中外文文献在 2 篇以上。
%%

@program{sysu-thesis,
  title  = {中山大学毕业论文Latex模板},
  author = {Songguang Chen and Junjie Huang  and Yongfeng, Wang},
  url    = {https://github.com/SYSU-SCC/sysu-thesis},
  year   = {2020}
}

@article{liu2011sift,
  title     = {Sift flow: Dense correspondence across scenes and its applications},
  author    = {Liu, Ce and Yuen, Jenny and Torralba, Antonio},
  journal   = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  volume    = {33},
  number    = {5},
  pages     = {978--994},
  year      = {2011},
  publisher = {IEEE}
}
@inproceedings{tighe2013finding,
  title     = {Finding things: Image parsing with regions and per-exemplar detectors},
  author    = {Tighe, Joseph and Lazebnik, Svetlana},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {3001--3008},
  year      = {2013}
}
@inproceedings{long2015fully,
  title     = {Fully convolutional networks for semantic segmentation},
  author    = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {3431--3440},
  year      = {2015}
}
@inproceedings{chen14semantic,
  title     = {Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs},
  author    = {Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille},
  booktitle = {ICLR},
  url       = {http://arxiv.org/abs/1412.7062},
  year      = {2015}
}
@incollection{hariharan2014simultaneous,
  title     = {Simultaneous detection and segmentation},
  author    = {Hariharan, Bharath and Arbel{\'a}ez, Pablo and Girshick, Ross and Malik, Jitendra},
  booktitle = {Computer vision--ECCV 2014},
  pages     = {297--312},
  year      = {2014},
  publisher = {Springer}
}
@misc{yang2019federated,
      title={Federated Machine Learning: Concept and Applications}, 
      author={Qiang Yang and Yang Liu and Tianjian Chen and Yongxin Tong},
      year={2019},
      eprint={1902.04885},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{fedavg,
      title={Communication-Efficient Learning of Deep Networks from Decentralized Data}, 
      author={H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Agüera y Arcas},
      year={2023},
      eprint={1602.05629},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{q-FFL,
      title={Fair Resource Allocation in Federated Learning}, 
      author={Tian Li and Maziar Sanjabi and Ahmad Beirami and Virginia Smith},
      year={2020},
      eprint={1905.10497},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{MA2022244,
title = {A state-of-the-art survey on solving non-IID data in Federated Learning},
journal = {Future Generation Computer Systems},
volume = {135},
pages = {244-258},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001686},
author = {Xiaodong Ma and Jia Zhu and Zhihao Lin and Shanxuan Chen and Yangjie Qin},
keywords = {Federated Learning, Non-IID data, Machine learning, Statistical heterogeneity},
abstract = {Federated Learning (FL) proposed in recent years has received significant attention from researchers in that it can enable multiple clients to cooperatively train global models without revealing private data. This training mode protects users’ privacy without violating the supervision, and aggregates scattered data to exert great potential. However, the data samples on each participating device of FL are usually not independent and identically distributed (IID), which leads to serious statistical heterogeneity challenges for FL. In this article, we analyze and establish the definition of non-IID data problems, and put forward a series of challenges that this problem may bring to FL. We classify existing methods to solve this problem from the researcher’s entry point and subsequent sub-methods, aiming to provide a comprehensive study for solving the problem of non-IID data in FL. Our research shows that non-IID data will not only reduce the performance of the FL model, but also damage the active participation of users in the FL process. Compared with methods based on data-side sharing, enhancement, and selection, it is more common for researchers to improve federated learning algorithms from models, algorithms, and frameworks to solve non-IID problems. To the best of our knowledge, although many efforts have been made to address the problem of non-IID data, there are currently few authoritative systematic reviews in this field and are not up-to-date. In this article, we will fill the gaps in FL and provide researchers with the state-of-the-art research results to solve non-IID problems in FL and promote the further implementation of FL.}
}
@article{田家会2022一种联邦学习中的公平资源分配方案,
  title={一种联邦学习中的公平资源分配方案},
  author={田家会 and 吕锡香 and 邹仁朋 and 赵斌 and 李一戈},
  journal={计算机研究与发展},
  number={059-006},
  year={2022},
}
@misc{mohri2019agnostic,
      title={Agnostic Federated Learning}, 
      author={Mehryar Mohri and Gary Sivek and Ananda Theertha Suresh},
      year={2019},
      eprint={1902.00146},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@INPROCEEDINGS{ ZhaoICASSP2022,
  author={Zhao, Zhiyuan and Joshi, Gauri},
  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A Dynamic Reweighting Strategy For Fair Federated Learning}, 
  year={2022},
  volume={},
  number={},
  pages={8772-8776},
  keywords={Training;Machine learning;Signal processing;Collaborative work;Data models;Speech processing;Optimization;federated learning;fairness;distributed optimization},
  doi={10.1109/ICASSP43922.2022.9746300}}
@INPROCEEDINGS{Wang2019IEEE,
  author={Wang, Guan and Dang, Charlie Xiaoqian and Zhou, Ziye},
  booktitle={2019 IEEE International Conference on Big Data (Big Data)}, 
  title={Measure Contribution of Participants in Federated Learning}, 
  year={2019},
  volume={},
  number={},
  pages={2597-2604},
  keywords={Data models;Machine learning;Biological system modeling;Predictive models;Computational modeling;Google;Approximation algorithms;federated learning;machine learning;deletion;shapley values},
  doi={10.1109/BigData47090.2019.9006179}}
@INPROCEEDINGS{Song2019IEEE,
  author={Song, Tianshu and Tong, Yongxin and Wei, Shuyue},
  booktitle={2019 IEEE International Conference on Big Data (Big Data)}, 
  title={Profit Allocation for Federated Learning}, 
  year={2019},
  volume={},
  number={},
  pages={2577-2586},
  keywords={Data models;Training;Machine learning;Indexes;Google;Servers;Task analysis;Federated Learning;Incentive Mechanism;Shapley Value},
  doi={10.1109/BigData47090.2019.9006327}}
@misc{finn2017modelagnostic,
      title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}, 
      author={Chelsea Finn and Pieter Abbeel and Sergey Levine},
      year={2017},
      eprint={1703.03400},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}@article{FedAvgOnNon-iid,
  doi = {10.48550/ARXIV.1806.00582},
  
  url = {https://arxiv.org/abs/1806.00582},
  
  author = {Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Federated Learning with Non-IID Data},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@ARTICLE{XiaoxinSu2023,
  author={Su, Xiaoxin and Zhou, Yipeng and Cui, Laizhong and Liu, Jiangchuan},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={On Model Transmission Strategies in Federated Learning With Lossy Communications}, 
  year={2023},
  volume={34},
  number={4},
  pages={1173-1185},
  keywords={Computational modeling;Training;Federated learning;Quantization (signal);Propagation losses;Packet loss;Convergence;Compression;federated learning;forward error correction;lossy communication;retransmission},
  doi={10.1109/TPDS.2023.3240883}}
@misc{tang2022virtual,
      title={Virtual Homogeneity Learning: Defending against Data Heterogeneity in Federated Learning}, 
      author={Zhenheng Tang and Yonggang Zhang and Shaohuai Shi and Xin He and Bo Han and Xiaowen Chu},
      year={2022},
      eprint={2206.02465},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}@ARTICLE{Miao2022,
  author={Miao, Yinbin and Liu, Ziteng and Li, Hongwei and Choo, Kim-Kwang Raymond and Deng, Robert H.},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Privacy-Preserving Byzantine-Robust Federated Learning via Blockchain Systems}, 
  year={2022},
  volume={17},
  number={},
  pages={2848-2861},
  keywords={Servers;Blockchains;Collaborative work;Computational modeling;Training;Resists;Privacy;Federated learning;poisoning attacks;fully homomorphic encryption;blockchain},
  doi={10.1109/TIFS.2022.3196274}}
@misc{tan2022fedproto,
      title={FedProto: Federated Prototype Learning across Heterogeneous Clients}, 
      author={Yue Tan and Guodong Long and Lu Liu and Tianyi Zhou and Qinghua Lu and Jing Jiang and Chengqi Zhang},
      year={2022},
      eprint={2105.00243},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{FedMGDA+,
      title={Federated Learning Meets Multi-objective Optimization}, 
      author={Zeou Hu and Kiarash Shaloudegi and Guojun Zhang and Yaoliang Yu},
      year={2023},
      eprint={2006.11489},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}