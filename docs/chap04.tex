\chapter{理论分析}
\label{cha:usage-example}

本节通过将q-FedAvg还有FedMGDA+和FedAvg算法进行比较，总结出共同点和差异点，综合起来对它们的公平性和效率进行分析。
\section{公平性分析}

首先，我们要验证：
对于上述的联邦数据，q-FedAvg和FedMGDA+ 目标相比于FedAvg是否能够产生更加公平的解决方案。根据图3.4和图3.5，再结合第三章节中实验结果的描述，我们可以得出结论：

1）q-FedAvg的所有目标组公平性都是明显高于基准测试的，稍微推广一下就是，只要q值初始化时是一个大于0的值，那么q > 0的目标组一定会有更低的标准差，这说明公平性更低，当然q值越大，公平性越大，当q足够大，模型也就是相当于只关注最差性能的AFL，单个最差设备的性能将会起到主导作用。

2）对于FedMGDA+的目标组，相对于q-FedAvg的实验结论，这边的不确定性比较明显：不是所有目标组都在公平性上表现更佳，而且随着通信轮数增加，标准差值波动的幅度非常的大，哪怕是公平性最低的$\eta$ = 1.0目标在特定的通信轮数中的标准差甚至很接近$\eta$ = 0.1这个公平性最高的组。所以我们这里提到的公平性最低与最高都是针对于一个大体平均值而言的，波动幅度实在太大异质性我们无法像q-FedAvg那样只调整超参数就能灵活地调控公平性的程度，因为我们无法控制更新量幅度的变化大小，也无法控制梯度合并权重后计算出的公共梯度的变化大小。

这两组实验结果都是和背后的理论相佐证的，根据二者分别的公式，我们就可以判断q-FedAvg是在通信阶段简单初始化q值就能对权重分配进行灵活分配从而调控联邦模型的公平性，因为二者显然是波动幅度小的正比关系。然而对于FedMGDA+，情况则没有那么简单了，在客户端更新梯度以及归一化$\eta^*$的过程中经过梯度合并权重后计算出的公共梯度的变化程度已经是不可精确地预测了，但总体而言，可以得出一个初步结论——忽略波动幅度过大造成的偶然性来看，全局步长$\eta$ = 0.1更小时，公平性更高，但是这个初步结论并不是很严谨，还是需要进一步实验去验证和考量。


\section{算法的效率分析}
在此部分，我们将对 q-FedAvg、 FedMGDA+和基准测试FedAvg的效率进行分析，主要从测试精度和收敛速度两个方面进行评估分析。

首先，我们要判断这个算法是否高效，就是判断这个算法是不是在更少的通信轮数中更快的达到了收敛（收敛速度），以及大体的测试精度分布和收敛的结果值是不是表现得更加突出。
在第三章节的实验结果图3.4和图3.5中，我们可以发现：

1）在q-FedAvg中，q值越大，往往收敛的速度会快一点，例如q = 5.0最大的目标组收敛速度就是，但是这并不代表效率高，因为从图中可以看出尽管收敛的速度比较快一点，况且收敛速度也没有快很多，大体上还是比较相近的，但是在测试精度方面却落后了，前面已经说过q值某种程度相当于上用全局结果去换取公平性，q = 0.1组略微在测试精度上表现的更好，但是和基准测试在大体上相差不大，其他两组则是在测试精度上表现明显劣于基准测试。

综合来看，主要考虑到测试精度方面的显著差异，我们可以认为q = 0.1组的性能应该是最好的，兼顾了效率和公平性，并且基准测试联邦平均FedAvg就等价于q = 0的情况，在测试精度上表现应该是比较良好的，q = 0.1和q = 0这里比较接近，在不考虑公平性的情况下，效率也可以说是大体一致，但根据实验结果可以推断，当q值逐渐增大，总体的测试精度会不断下降，计算的复杂性变大，收敛需要的通信轮数变多，所以效率也会随之降低。

2）在FedMGDA+中，尽管由于理论本身就决定了全局步长$\eta$对更新的公共梯度带来的波动幅度变化应该是比较大的，我们还是可以从实验结果中得出一些结论——总体而言，$\eta$越大的组在测试精度上表现比较越好，平均来看有着更高的测试集准确率和验证集准确率，又由于波动幅度大的原因，在我们有限的通信轮数实验中，未能观察到有任一目标组达到近似收敛的情况，故而我们可以不考虑收敛速度这个评价指标，仅从测试精度角度出发来看，FedMGDA+的模型性能受全局步长$\eta$影响，在我们实验的范围[0.1,1]中，$\eta$越大则效率越高。这里还可以发现波动幅度的大小也受$\eta$的影响，$\eta$值越大，虽然测试精度总体呈上升趋势，但是同样波动幅度也在增大，导致了更大的不确定性，这也是我们需要进一步实验去探究和思考的方向。




