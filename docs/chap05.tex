%% chapter 5 dataset, network structure, experiment and result
\chapter{结论}
\label{cha:experiment}
\section{本文工作总结}
\subsection{本文动机}


从绪论开始，我们简要介绍了联邦学习FL的概念，从FL遇到的主要四种类型的问题和近期研究给出的解决思路和方案，再展开讲到联邦学习经典算法FedAvg。
接着从此处出发引入联邦学习公平性的概念：一种是贡献公平性，按照参与者的贡献给予相应奖励，让高贡献的参与者感到公平；另一种是均衡公平性，尽可能让每个客户端都有机会参与到联邦模型训练中来，让低贡献的参与者感到公平。
我们要研究的领域聚焦于后者，即如何不损害低贡献参与者的利益，保证联邦模型的均衡公平性。然后给出问题陈述，并给出目前用于提高联邦学习公平性的两种算法q-FedAvg与FedMGDA+的架构,并将它们与FedAvg进行了实验对比，从得到的实验结果来分析两种算法模型如何对公平性的影响，并且对公平性和效率进行分析。


\subsection{实验方法与分析}

q-FedAvg是对FedAvg的推广，通过在通信阶段引入超参数q，可以通过对q初始值的调整，在保持整体的测试精度足够高的情况下，按照应用所需进行公平性调度（q与公平性是正关系），给予损失值大的客户端更高权重，使得测试精度标准差更小，提高了联邦学习的公平性。
FedMGDA+也是对FedAvg的推广，受多目标优化中的mgda算法所启发，将参数更新应用于联邦学习的聚合阶段，在聚合阶段把直接加权平均改成梯度合并权重——先计算本地模型归一化的更新梯度，根据该梯度在权重在邻域计算聚合权重，聚合权重去加权平均模型更新量，再乘以用全局步长$\eta$进行模型更新。我们控制全局步长$\eta$从而维护不同参与者梯度的一致性，保证更新出来的公共梯度是朝着益于公平性的方向进行的，然而由于公共梯度变化值较大，也导致测试精度和公平性波动幅度较大，暂无法灵活地调控所需公平性程度。

\subsection{创新点}

从联邦学习的两个阶段:通信阶段和聚合阶段去探讨了提高联邦学习公平性的方法，并且总结了二者的共同点和差异，对于研究提高联邦学习算法公平性的视角更加开阔，也拓宽了一些思路。

\section{展望}


\subsection{主要缺点}

q-FedAvg算法和FedMGDA+都没有事先确定好超参数的值,只能多做几组对比实验,选出其中表现好的目标,效率非常低，而且找到的对应值还可能牺牲了测试精度，因为过多地提升了性能差的参与者的权重;
收敛速度比较慢，尤其当遇到高度异构的本地数据时。
实验的设置有欠缺，目标组的数量不够，也没有进行针对不同客户端数量情况的对比实验，需要进一步完善实验的细节。

\subsection{优化目标}

首先需要学习更好的初始化值，事先就选择出合适的超参数值，使得两个算法可以以公平的方式快速解决未见的任务，即减少个性化模型精度分布的方差。

特别对于FedMGDA+而言，还要思考如何减少性能波动幅度，过大的波动幅度不利于我们进行更大规模的训练，这可能导致部分参与者得到的模型表现较差，就会造成不公平的现象，要提升公平性也同时要注意控制好测试精度的波动幅度，这方面有待进一步研究。

其次，还要提升针对高度异构数据场景下的性能，提升其收敛速度和精度。
\newline