\chapter{相关工作}

\label{cha：sysu-thesis-contents-format-requirement}


% TODO 引用
该部分将介绍研究思路和理论公式，以及具体的实验过程。

\section{相关技术前置知识}

\subsection{经典算法架构FedAvg}

在该部分中，概述了一些与论文直接相关的最新研究，将论文的贡献置于整个背景之中。最初，H. Brendan McMahan等人在2017年首次提出了一项联邦学习(FL)算法，即“联邦平均”(Federated Averaging，简称为FedAvg)\cite{FedAvg}。 值得注意的是，在联邦学习中，相较于传统的数据中心处理模式，客户端本地计算与服务器聚合的计算量都相对较小，但客户端与服务器的通信成本较高。因此，本文提出两种降低通信成本的方法：1. 增加并行性，即使用更多客户端独立训练模型；2. 增加每个客户端的计算量。 FedAvg采用同步更新策略，需要进行多轮迭代。固定数量的K个客户端每轮参与，每个客户端拥有固定的本地数据集。每一轮开始时，随机选择比例为C的客户端子集，并向其发送当前的全局算法状态（例如当前的模型参数）。

公式表示即是
\begin{equation}
\min_{w\in\mathbb{R}^d}f(w)\quad\text{where}\quad f(w)\stackrel{\text{def}}{=}\frac{1}{n}\sum_{i=1}^nf_i(w). 
\end{equation}

在机器学习问题中，文中通常定义损失函数$f_i(w) = (x_i, y_i; w)$即用模型参数$w$对样本$(x_i,y_i)$进行预测的损失。文中假设数据被划分到$K$个客户端上，其中$P k$是$K$个客户端上数据点的索引集合,
\begin{equation}
    n_k=|P_k|   
\end{equation}
表示客户端
$K$上的数据点数量。因此，文中可以将目标函数（1）重写为
\begin{equation}
f(w)=\sum_{k=1}^K\frac{n_k}{n}F_k(w)\quad\text{where}\quad F_k(w)=\frac{1}{n_k}\sum_{i\in\mathcal{P}_k}f_i(w).    
\end{equation}
如果分区$P_k$是通过随机均匀地在客户端之间分配训练样本而形成的，那么会有$\mathbb{E}_{\mathcal{P}_k}[F_k(w)]=f(w)$。这就是分布式优化算法通常所做的独立同分布（IID）假设；文中将不满足这一假设的情况（即$F_k$可能是对$f$的任意糟糕的近似）称为非IID设置.
FedAvg的过程使用伪代码表达如下：
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{image/chap02/FedAvg.png}
\caption{FedAvg算法流程}
\label{fig：FedAvg}
\end{figure}

\subsection{联邦学习公平性定义}
目前联邦学习有三个主要的约束，公平性、鲁棒性和隐私性，在我们目前的联邦学习前沿领域也没有提出能同时满足三种约束的算法，在本文中我们将主要探究关于公平性的提升。
首先介绍公平性的联邦学习公平性主要分为两方面：

1）均衡公平性：是在强调每个客户端都要有一定机会去参与联邦训练，而不是只有强势的客户端（一般有更高的性能和容量从而在权重更新中占主导作用）垄断了机会。尽管联邦学习有助于保护数据隐私，但面临着数据异构性的挑战，表现为客户端数据分布的显著差异。全局模型难以达到最优，只能接近于集中训练的最优性能，难以满足所有客户的需求。
而现实中训练结果往往受强势的客户端引导，对强势方有利，而劣势方表现不佳。为了消除这种不公平，提出了客户间的均衡公平性。我们可以从FedAvg的训练流程中得到启发——服务器会随机选择部分客户端，被选中的客户端才会参与本地更新和模型上传，服务器根据这些客户端上传的权重聚合全局模型。显然，我们要提升公平性可以从三个角度进行切入：客户选择、权重聚合和本地更新三方面，通过关注弱势客户端代表不足（即表现性能较差的客户端数量过少)、公平分配聚合权重和训练个性化本地模型来提高全局模型，可以使各个客户端都有一定的机会、权重、和个性去参与全局的联邦训练，实现均衡公平性。
本文中的方法就是在分配聚合权重部分去修改联邦平均算法，详细的讲解在2.1.3节中。

2）贡献公平性：代表弱势客户端利益的均衡公平性更加注重结果公平，它们希望的结果是最终模型在不同客户端上的表现尽可能相近，也就是最终模型可以适用于参与训练的所有客户端，普适性比较强。但是对于强势的客户端而言，他们花费了更多算力，做出更多的贡献，如果收到和在弱势的客户端表现几近相同的全局 模型，这是不公平的，不仅影响了他们的积极性，同时也带来了弱势客户端的"蹭模型"问题，不利于我们的联邦学习可持续发展。

所以，我们必须公正地评估每个客户对全局模型的贡献，同时保护他们的隐私。
目前有许多评估客户端贡献的技术：Shapley值、区块链、契约机制、声誉、博弈论和拍卖机制等等，也有借助强化学习和区块链等先进技术作为节点选择、评估贡献和提高鲁棒性的辅助手段。
其中，Shapley值的应用非常广泛。一般采用Shapley值来评估贡献，它可以帮助我们合理计算各个客户端与其贡献相对应的精确报酬值，从而促进资源的公平分配。不仅如此，在机器学习领域中，基于Shapley值的测量方法还经常用于评估数据对模型的性能。

总结联邦学习的两种公正性：一是均衡公平性，代表着平衡公正，即“所有人机会均等”，更关注“低绩效”用户（弱势客户端），为那些贡献较低的数据参与方提供更多机会；二是贡献公平，即"按劳分配"，通过贡献指数分配利润，鼓励贡献值大的数据参与方（强势客户端）起主导作用。

\subsection{相关的算法}
\subsubsection{算法1：q-FedAvg}
在这项提升联邦学习公平性的工作中,文章参考了Tian Li等人在2019年提出的文献Fair Resource Allocation in Federated Learning\cite{q-FFL}。文中提出了优化目标q-FFL,这是一个新颖的优化目标，受到无线网络公平资源分配和相关负载平衡算法的影响，q-FFL简单来说就是通过超参数q来进行聚合加权损失最小化，以保障高损失设备获得更高的相对权重，从而提升联邦学习的公平性，具体定义将在第三章的问题陈述中给出。

q-FedAvg实现的公平性是第一种均衡公平性，属于调整聚合权重的方法。类似论文2019年ICML会议上提出的Agnostic Federated Learning\cite{mohri2019agnostic}仅仅专注于优化表现最差的参与者;2022年ICASSP会议\cite{ZhaoICASSP2022}上提出了一种用于公平联邦学习的动态重新加权策略，即采用q-FFL目标中的损失放大机制来替代简单的权重再分配机制。这种策略通过给损失较大的客户端分配更大的权重，从而增加对这些客户端的惩罚力度;2022年田家会等人提出了α-FedAvg算法\cite{田家会2022一种联邦学习中的公平资源分配方案}，在联邦学习中采用Jain's Index度量公平性，有助于减少准确度的差异。
	
 然而，q-FedAvg无法事先确定最佳q值，且很难在数据异构的情况下收敛。而由损失较小的用户（高贡献客户端）来弥补损失较大的部分，也就是说自己贡献值更高，但是得到的联邦模型的收益却更低，这对于那些强势客户端的用户是不公平的。
\begin{figure}[htb]
	
	\begin{minipage}{0.49\linewidth}%可修改0.49为其他比例，调整大小
		\vspace{3pt}
		\centerline{\includegraphics[width=\textwidth]{image//chap02/qffl.png}}
	\end{minipage}
	\begin{minipage}{0.49\linewidth}
		\vspace{3pt}
		\centerline{\includegraphics[width=\textwidth]{image//chap02/qffl-accuracy.png}}
	\end{minipage}
 
	\caption{q-FedAvg与FedAvg的对比}
\end{figure}

首先我们比较一下q-FedAvg与FedAvg的性能，在图2.2中检验了通过基线方法（FedAvg）与 q-FedAvg 训练的模型的测试集准确率test\_accuracy的分布以及对应的测试集损失值test\_loss的分布。实验横坐标是通信轮数round，纵坐标则是test\_accuracy/test\_loss,这里我们设置30个客户端一共运行了20个通信轮次。
这里我们把q初始值设置为1.0，可以发现q-FedAvg在测试准确率和损失值方面都不如FedAvg，但收敛的速度总体而言相对快一点。
考虑到我们使用的合成数据集的适用性，q-FedAvg之所以低于FedAvg的原因很大可能是q的初始值不对，需要多组实验对照去决定合适的q值，我们要针对该数据集去找出合适的q值，直至确保q-FedAvg高效性和公平性都能超过现有基线（在第三章节中给出详细过程）。

应用的场景：当由于设备之间存在数据的差异，某些设备上的模型准确率相当差。这时就通过采用q-FFL目标，在通信阶段进行简单的一个修改—超参数q引入来控制各个设备的权重，通过动态调整之后，就可以在保持整体平均准确度不变的同时，确保网络中服务质量更加公平和均匀。在此基础上，使用q-FedAvg可以灵活地最小化q-FFL目标结果，动态地调整参数来对所需的公平性程度进行调整。

\subsubsection{算法2：FedMGDA+}
受多目标优化问题的启发，Zeou Hu等人在2023年提出了
FedMGDA+框架\cite{FedMGDA+}。
FedMGDA+就把多目标优化中的mgda（Multi-Gradient Descent Algorithm,多重梯度下降算法)应用在联邦学习领域当中。
FedMGDA+的目标就是保证模型能收敛到帕累托稳定解。为了确保公平性，我们不能让联邦模型对少部分的参与者"偏爱有加"，乃至于作出"损人利己"的行为。
因为当问题的解已经达到了帕累托最优状态时，如果我们要增加某个目标的收益，这一定伴随着其他的目标的收益受损。

在联邦平均FedAvg模型的训练过程中，由于它的优化目标是降低全局损失，因此它往往在实际应用当中存在通过牺牲某个参与者的表现来提升全局结果的操作。
然而这可能导致不公平现象发生：为了优化全局损失到更优的解，某个参与者的性能非常糟糕，那么联邦学习传回的模型对它就无益，这将降低参与者的积极性，从而也会影响联邦学习的模型聚合。
不同于FedAvg那样预先设置好合并权重，FedMGDA+根据已经获得的梯度信息，来计算合并权重。

使用参数${\lambda}_0$和$\epsilon$去限制${\lambda}_t^*$的优化方向，详细公式流程见第三章节。
FedMGDA+还提升了联邦模型的鲁棒性，因为它引入了梯度normalization去减少恶意梯度对模型聚合带来的影响。

% TODO：增加引用例子。